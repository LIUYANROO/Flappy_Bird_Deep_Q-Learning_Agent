{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0110dd-cb47-4a09-aa81-c1fcc842df58",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hyperparameters-Copy1.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 280\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# To run the agent and start training:\u001b[39;00m\n\u001b[1;32m    279\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflappybird1\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your actual hyperparameter set name\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(hyperparameter_set\u001b[38;5;241m=\u001b[39mhyperparameters)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m#agent.run(is_training=True, render=False)  # Train without rendering\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# After training is done, show the final game\u001b[39;00m\n\u001b[1;32m    284\u001b[0m agent\u001b[38;5;241m.\u001b[39mrun(is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, hyperparameter_set)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hyperparameter_set):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparameters-Copy1.yml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     41\u001b[0m         all_hyperparameter_sets \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(file)\n\u001b[1;32m     42\u001b[0m         hyperparameters \u001b[38;5;241m=\u001b[39m all_hyperparameter_sets[hyperparameter_set]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hyperparameters-Copy1.yml'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import flappy_bird_gymnasium\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "import pygame\n",
    "\n",
    "from experience_replay import ReplayMemory\n",
    "from dqn import DQN\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import count\n",
    "\n",
    "# For printing date and time\n",
    "DATE_FORMAT = \"%m-%d %H:%M:%S\"\n",
    "\n",
    "# Directory for saving run info\n",
    "RUNS_DIR = \"runs\"\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "\n",
    "# 'Agg': used to generate plots as images and save them to a file instead of rendering to screen\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'  # Force CPU (optional depending on your system)\n",
    "\n",
    "# Deep Q-Learning Agent\n",
    "class Agent():\n",
    "\n",
    "    def __init__(self, hyperparameter_set):\n",
    "        with open('hyperparameters.yml', 'r') as file:\n",
    "            all_hyperparameter_sets = yaml.safe_load(file)\n",
    "            hyperparameters = all_hyperparameter_sets[hyperparameter_set]\n",
    "\n",
    "        self.hyperparameter_set = hyperparameter_set\n",
    "\n",
    "        # Hyperparameters (adjustable)\n",
    "        self.env_id = hyperparameters['env_id']\n",
    "        self.learning_rate_a = hyperparameters['learning_rate_a']        # learning rate (alpha)\n",
    "        self.discount_factor_g = hyperparameters['discount_factor_g']      # discount rate (gamma)\n",
    "        self.network_sync_rate = hyperparameters['network_sync_rate']      # number of steps the agent takes before syncing the policy and target network\n",
    "        self.replay_memory_size = hyperparameters['replay_memory_size']     # size of replay memory\n",
    "        self.mini_batch_size = hyperparameters['mini_batch_size']        # size of the training data set sampled from the replay memory\n",
    "        self.epsilon_init = hyperparameters['epsilon_init']           # 1 = 100% random actions\n",
    "        self.epsilon_decay = hyperparameters['epsilon_decay']          # epsilon decay rate\n",
    "        self.epsilon_min = hyperparameters['epsilon_min']            # minimum epsilon value\n",
    "        self.stop_on_reward = hyperparameters['stop_on_reward']         # stop training after reaching this number of rewards\n",
    "        self.fc1_nodes = hyperparameters['fc1_nodes']\n",
    "        self.env_make_params = hyperparameters.get('env_make_params', {}) # Get optional environment-specific parameters, default to empty dict\n",
    "        self.enable_double_dqn = hyperparameters['enable_double_dqn']      # double dqn on/off flag\n",
    "        self.enable_dueling_dqn = hyperparameters['enable_dueling_dqn']     # dueling dqn on/off flag\n",
    "\n",
    "        # Neural Network\n",
    "        self.loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "        self.optimizer = None                # NN Optimizer. Initialize later.\n",
    "\n",
    "        # Path to Run info\n",
    "        self.LOG_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.log')\n",
    "        self.MODEL_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.pt')\n",
    "        self.GRAPH_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.png')\n",
    "\n",
    "    def run(self, is_training=True, render=False):\n",
    "        if is_training:\n",
    "            # Initialize variables for training logging\n",
    "            training_rewards = []\n",
    "            start_time = datetime.now()\n",
    "            last_graph_update_time = start_time\n",
    "\n",
    "            log_message = f\"{start_time.strftime(DATE_FORMAT)}: Training starting...\"\n",
    "            print(log_message)\n",
    "            with open(self.LOG_FILE, 'w') as file:\n",
    "                file.write(log_message + '\\n')\n",
    "\n",
    "        # Create instance of the environment.\n",
    "        env = gym.make(self.env_id, render_mode='human' if render else None, **self.env_make_params)\n",
    "\n",
    "        # Number of possible actions\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Get observation space size\n",
    "        num_states = env.observation_space.shape[0]  # Expecting type: Box(low, high, (shape0,), float64)\n",
    "\n",
    "        # List to keep track of rewards collected per episode.\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        # Create policy and target network.\n",
    "        policy_dqn = DQN(num_states, num_actions, self.fc1_nodes, self.enable_dueling_dqn).to(device)\n",
    "\n",
    "        # Load the pre-trained model if available\n",
    "        if os.path.exists(self.MODEL_FILE):\n",
    "            print(\"Loading pre-trained model...\")\n",
    "            policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n",
    "            policy_dqn.eval()  # Set to evaluation mode\n",
    "\n",
    "        if is_training:\n",
    "            # Initialize epsilon\n",
    "            epsilon = self.epsilon_init\n",
    "\n",
    "            # Initialize replay memory\n",
    "            memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "            # Create the target network and make it identical to the policy network\n",
    "            target_dqn = DQN(num_states, num_actions, self.fc1_nodes, self.enable_dueling_dqn).to(device)\n",
    "            target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "            # Policy network optimizer.\n",
    "            self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "            # List to keep track of epsilon decay\n",
    "            epsilon_history = []\n",
    "\n",
    "            # Track number of steps taken.\n",
    "            step_count = 0\n",
    "\n",
    "            # Track best reward\n",
    "            best_reward = -9999999\n",
    "        else:\n",
    "            # Load learned policy\n",
    "            policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n",
    "\n",
    "            # switch model to evaluation mode\n",
    "            policy_dqn.eval()\n",
    "\n",
    "        # Train indefinitely, manually stop the run when you are satisfied\n",
    "        for episode in count():\n",
    "\n",
    "            state, _ = env.reset()  # Initialize environment. Reset returns (state,info).\n",
    "            state = torch.tensor(state, dtype=torch.float, device=device)  # Convert state to tensor directly on device\n",
    "\n",
    "            terminated = False      # True when agent reaches goal or fails\n",
    "            episode_reward = 0.0    # Used to accumulate rewards per episode\n",
    "\n",
    "            while not terminated and episode_reward < self.stop_on_reward:\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if is_training and random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                    action = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
    "\n",
    "                # Execute action\n",
    "                new_state, reward, terminated, truncated, info = env.step(action.item())\n",
    "\n",
    "                # Accumulate rewards\n",
    "                episode_reward += reward\n",
    "\n",
    "                # Convert new state and reward to tensors\n",
    "                new_state = torch.tensor(new_state, dtype=torch.float, device=device)\n",
    "                reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
    "\n",
    "                if render:\n",
    "                    self.show_frame(new_state)  # Show frame using pygame\n",
    "\n",
    "                if is_training:\n",
    "                    # Save experience into memory\n",
    "                    memory.append((state, action, new_state, reward, terminated))\n",
    "\n",
    "                    step_count += 1\n",
    "\n",
    "                # Move to next state\n",
    "                state = new_state\n",
    "\n",
    "            rewards_per_episode.append(episode_reward)\n",
    "\n",
    "            if is_training:\n",
    "                if episode_reward > best_reward:\n",
    "                    log_message = f\"{datetime.now().strftime(DATE_FORMAT)}: New best reward {episode_reward:0.1f} ({(episode_reward - best_reward) / best_reward * 100:+.1f}%) at episode {episode}, saving model...\"\n",
    "                    print(log_message)\n",
    "                    with open(self.LOG_FILE, 'a') as file:\n",
    "                        file.write(log_message + '\\n')\n",
    "\n",
    "                    torch.save(policy_dqn.state_dict(), self.MODEL_FILE)\n",
    "                    best_reward = episode_reward\n",
    "\n",
    "                if len(memory) > self.mini_batch_size:\n",
    "                    mini_batch = memory.sample(self.mini_batch_size)\n",
    "                    self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                    epsilon = max(epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "                    epsilon_history.append(epsilon)\n",
    "\n",
    "                    if step_count > self.network_sync_rate:\n",
    "                        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                        step_count = 0\n",
    "\n",
    "        # After training, render the final game state\n",
    "        if not is_training:\n",
    "            print(\"Training finished, displaying final game...\")\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "            done = False\n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(state.unsqueeze(0)).squeeze().argmax()\n",
    "                new_state, reward, done, truncated, info = env.step(action.item())\n",
    "                self.show_frame(new_state)\n",
    "                state = torch.tensor(new_state, dtype=torch.float, device=device)\n",
    "            print(\"Game over!\")\n",
    "\n",
    "    def show_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Display the current game frame using pygame.\n",
    "        If the frame is not an image, print its values instead.\n",
    "        \"\"\"\n",
    "        if isinstance(frame, torch.Tensor):\n",
    "            frame = frame.cpu().numpy()  # Convert to numpy if it's a tensor\n",
    "\n",
    "        if len(frame.shape) == 3 and frame.shape[2] == 3:\n",
    "            # Normal image (height, width, 3)\n",
    "            height, width, _ = frame.shape\n",
    "        elif len(frame.shape) == 2:\n",
    "            # Grayscale image (height, width)\n",
    "            height, width = frame.shape\n",
    "            frame = np.stack([frame] * 3, axis=-1)  # Convert grayscale to 3-channel by repeating the grayscale values\n",
    "        else:\n",
    "            # If it's not an image, just print the frame contents\n",
    "            print(\"State vector:\", frame)\n",
    "            return\n",
    "\n",
    "        # Ensure pygame is initialized\n",
    "        if not pygame.get_init():\n",
    "            pygame.init()\n",
    "\n",
    "        # Create display window\n",
    "        screen = pygame.display.set_mode((width, height))  # Set mode using width and height\n",
    "        pygame.display.set_caption(\"Flappy Bird\")\n",
    "\n",
    "        frame = np.swapaxes(frame, 0, 1)  # Convert from (height, width, channels) to (width, height, channels)\n",
    "        frame = pygame.surfarray.make_surface(frame)  # Create surface for pygame\n",
    "\n",
    "        screen.blit(frame, (0, 0))  # Blit the frame to the screen\n",
    "        pygame.display.update()  # Update the display\n",
    "        pygame.time.delay(10)  # Delay to make the game viewable and slow down\n",
    "\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "        states, actions, new_states, rewards, terminations = zip(*mini_batch)\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        new_states = torch.stack(new_states)\n",
    "        rewards = torch.stack(rewards)\n",
    "        terminations = torch.tensor(terminations).float().to(device)\n",
    "\n",
    "        # Ensure the shapes match for loss calculation\n",
    "        current_q_values = policy_dqn(states).gather(1, actions.unsqueeze(1))  # Shape: (batch_size, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.enable_double_dqn:\n",
    "                best_actions_from_policy = policy_dqn(new_states).argmax(dim=1)\n",
    "                target_q_values = target_dqn(new_states).gather(1, best_actions_from_policy.unsqueeze(1))  # Shape: (batch_size, 1)\n",
    "            else:\n",
    "                target_q_values = target_dqn(new_states).max(dim=1)[0].unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Compute expected Q-values\n",
    "        expected_q_values = rewards + (self.discount_factor_g * target_q_values * (1 - terminations))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(current_q_values, expected_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# To run the agent and start training:\n",
    "hyperparameters = 'flappybird1'  # Replace with your actual hyperparameter set name\n",
    "agent = Agent(hyperparameter_set=hyperparameters)\n",
    "#agent.run(is_training=True, render=False)  # Train without rendering\n",
    "\n",
    "# After training is done, show the final game\n",
    "agent.run(is_training=False, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443af67c-54e6-4e1b-96d5-deae8f62d0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333406e1-22a1-4bf7-960c-ce09fcf0eed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
